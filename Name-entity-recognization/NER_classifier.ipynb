{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_bc_GONGCHUNZHU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW_GRAHEDpf7"
      },
      "source": [
        "### import custom data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uDaKmu4DpgA",
        "outputId": "9405f0a8-cd8c-4196-93a5-3db60240432f"
      },
      "source": [
        "import copy\n",
        "import pprint\n",
        "pp = pprint.PrettyPrinter()\n",
        "from functools import partial\n",
        "from random import randint\n",
        "\n",
        "# evaluate CRF model\n",
        "!pip install sklearn-crfsuite\n",
        "from sklearn_crfsuite import metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_path = 'wnut17train.conll'\n",
        "dev_path = 'emerging.dev.conll'\n",
        "test_path = 'emerging.test.conll'\n",
        "\n",
        "\n",
        "def file2data(path, test=False):\n",
        "    with open(path, 'r') as file:\n",
        "        sents = file.read()\n",
        "        # split sents\n",
        "        split_key = '\\n\\n' if test else '\\n\\t\\n'\n",
        "        sents = sents.split(split_key)\n",
        "        sents = [s.split('\\n') for s in sents]\n",
        "        for i in range(len(sents)):\n",
        "            sents[i] = [tuple(word.split('\\t')) for word in sents[i]]\n",
        "#             sents[i] = [tuple([pair[0], pair[1].split(',')[0]]) for pair in sents[i]]\n",
        "        \n",
        "        # last element is None\n",
        "        sents.pop()\n",
        "    return sents\n",
        "\n",
        "train_data = file2data(train_path)\n",
        "dev_data = file2data(dev_path, test=True)\n",
        "test_data = file2data(test_path, test=True)\n",
        "print(len(train_data))\n",
        "print(len(dev_data))\n",
        "print(len(test_data))\n",
        "print(train_data[0])\n",
        "print(test_data[0])\n",
        "print()\n",
        "\n",
        "\n",
        "# check labels\n",
        "train_labels = set([word[1] for sent in train_data for word in sent])\n",
        "dev_labels = set([word[1] for sent in dev_data for word in sent])\n",
        "test_labels = set([word[1] for sent in test_data for word in sent])\n",
        "print(train_labels)\n",
        "print()\n",
        "print(dev_labels)\n",
        "print()\n",
        "print(test_labels) # multi labels"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (1.15.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (4.41.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite) (0.8.7)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.7 sklearn-crfsuite-0.3.6\n",
            "2394\n",
            "1009\n",
            "1287\n",
            "[('@paulwalk', 'O'), ('It', 'O'), (\"'s\", 'O'), ('the', 'O'), ('view', 'O'), ('from', 'O'), ('where', 'O'), ('I', 'O'), (\"'m\", 'O'), ('living', 'O'), ('for', 'O'), ('two', 'O'), ('weeks', 'O'), ('.', 'O'), ('Empire', 'B-location'), ('State', 'I-location'), ('Building', 'I-location'), ('=', 'O'), ('ESB', 'B-location'), ('.', 'O'), ('Pretty', 'O'), ('bad', 'O'), ('storm', 'O'), ('here', 'O'), ('last', 'O'), ('evening', 'O'), ('.', 'O')]\n",
            "[('&', 'O'), ('gt', 'O'), (';', 'O'), ('*', 'O'), ('The', 'O'), ('soldier', 'O'), ('was', 'O'), ('killed', 'O'), ('when', 'O'), ('another', 'O'), ('avalanche', 'O'), ('hit', 'O'), ('an', 'O'), ('army', 'O'), ('barracks', 'O'), ('in', 'O'), ('the', 'O'), ('northern', 'O'), ('area', 'O'), ('of', 'O'), ('Sonmarg', 'B-location'), (',', 'O'), ('said', 'O'), ('a', 'O'), ('military', 'O'), ('spokesman', 'O'), ('.', 'O')]\n",
            "\n",
            "{'B-group', 'B-location', 'O', 'I-creative-work', 'B-product', 'I-product', 'I-location', 'B-creative-work', 'B-person', 'I-corporation', 'I-person', 'I-group', 'B-corporation'}\n",
            "\n",
            "{'B-group', 'I-creative-work', 'B-location', 'O', 'B-product', 'I-product', 'I-location', 'B-creative-work', 'B-person', 'I-group', 'I-person', 'I-corporation', 'B-corporation'}\n",
            "\n",
            "{'I-creative-work,I-corporation', 'B-creative-work,B-corporation', 'B-corporation,B-group', 'B-product,B-group', 'I-creative-work,B-person', 'B-product', 'B-corporation,I-person,I-group', 'I-product', 'I-corporation,I-group', 'B-creative-work,I-product', 'I-location', 'B-creative-work', 'B-creative-work,B-product,B-corporation', 'B-creative-work,B-person,B-product', 'I-creative-work,I-person,I-product,I-group', 'B-creative-work,B-location,B-corporation,B-product', 'B-creative-work,B-product,B-corporation,B-group', 'I-creative-work,B-location', 'B-creative-work,B-person,B-product,B-corporation', 'I-person,I-product', 'I-creative-work,I-person', 'I-creative-work,I-product,I-corporation', 'I-creative-work,I-product,B-corporation', 'B-person,I-product', 'I-creative-work,I-corporation,I-group', 'I-creative-work,B-corporation', 'I-creative-work,I-product', 'B-creative-work,B-person,B-corporation,B-group', 'I-creative-work,B-group', 'B-location', 'O', 'I-creative-work,I-location', 'B-creative-work,B-location', 'I-creative-work,B-location,B-product', 'I-product,I-group', 'I-person,I-group', 'I-creative-work,I-location,I-corporation', 'B-location,B-group', 'B-creative-work,B-location,B-corporation', 'I-location,B-group', 'I-creative-work,I-person,I-corporation,I-group', 'I-creative-work,B-person,B-product', 'B-creative-work,B-product', 'B-corporation,B-person,B-group', 'I-person,B-product', 'B-corporation', 'B-creative-work,B-corporation,B-group', 'B-corporation,B-product', 'I-creative-work', 'B-corporation,B-product,B-group', 'B-corporation,B-person,B-location', 'B-creative-work,B-location,B-product', 'B-corporation,B-person', 'I-corporation,B-location', 'B-creative-work,B-person,B-product,B-group', 'B-creative-work,B-group', 'I-creative-work,I-product,I-group', 'I-creative-work,B-person,I-corporation', 'B-creative-work,B-product,B-group', 'I-creative-work,I-group', 'B-person,B-product', 'I-corporation,I-location', 'B-person,B-location,B-group', 'I-corporation,I-product', 'B-creative-work,B-location,B-group', 'B-corporation,B-location', 'B-group', 'B-creative-work,B-person,B-group', 'B-corporation,B-location,B-product', 'B-person,B-location', 'I-group', 'B-creative-work,B-person', 'B-person', 'I-corporation', 'I-person', 'I-creative-work,B-corporation,I-group', 'B-creative-work,B-person,B-corporation', 'I-creative-work,I-person,I-product', 'I-location,I-group', 'I-corporation,I-person', 'B-creative-work,I-product,B-group', 'B-person,B-group'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh6qToBNRro4"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWIshaAS1mTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257fdce3-36a8-4d77-fd6a-df5cbf290e3b"
      },
      "source": [
        "# functions of sentence representations for sequence labelling\n",
        "def sent2labels(sent):\n",
        "    return [items[-1].split(',')[0] for items in sent]\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [items[0] for items in sent]\n",
        "\n",
        "# sentence representations for sequence labelling\n",
        "def convert_labels_to_inds(sent_labels, label_2_id):\n",
        "  return [label_2_id[label] for label in sent_labels]\n",
        "\n",
        "def transform_tokens_labels(data):\n",
        "    tokens = [sent2tokens(s) for s in data]\n",
        "    labels = [sent2labels(s) for s in data]\n",
        "    id_2_label = list(set([label for sent in labels for label in sent]))\n",
        "    print(\"Number of unique labels in training data:\", len(id_2_label))\n",
        "    label_2_id = {label:i for i, label in enumerate(id_2_label)}\n",
        "    label_inds = [convert_labels_to_inds(sent_labels, label_2_id) for sent_labels in labels]\n",
        "    return tokens, label_inds, labels\n",
        "\n",
        "train_sent_tokens, train_label_inds, train_labels = transform_tokens_labels(train_data)\n",
        "train_id_2_label = list(set([label for sent in train_labels for label in sent]))\n",
        "\n",
        "dev_sent_tokens, dev_label_inds, dev_labels = transform_tokens_labels(dev_data)\n",
        "test_sent_tokens, test_label_inds, test_labels = transform_tokens_labels(train_data)\n",
        "\n",
        "print(train_sent_tokens[0])\n",
        "print(train_labels[0])\n",
        "print(train_label_inds[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique labels in training data: 13\n",
            "Number of unique labels in training data: 13\n",
            "Number of unique labels in training data: 13\n",
            "['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-location', 'I-location', 'I-location', 'O', 'B-location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 6, 6, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m963KaxfbNbs"
      },
      "source": [
        "window_size = 2\n",
        "\n",
        "# converting tokenized sentence lists to vocabulary indices\n",
        "id_2_word = list(set([token for sent in train_sent_tokens for token in sent])) + [\"<pad>\", \"<unk>\"]\n",
        "word_2_id = {w:i for i,w in enumerate(id_2_word)}\n",
        "\n",
        "def convert_tokens_to_inds(sentence, word_2_id):\n",
        "    return [word_2_id.get(t, word_2_id[\"<unk>\"]) for t in sentence]\n",
        "\n",
        "# padding for windows\n",
        "def pad_sentence_for_window(sentence, window_size=2, pad_token=\"<pad>\"):\n",
        "    return [pad_token]*window_size + sentence + [pad_token]*window_size \n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN7BXIZVueq5"
      },
      "source": [
        "# Batching sentences together with a DataLoader\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def my_collate(data, window_size, word_2_id):\n",
        "    \"\"\"\n",
        "    For some chunk of sentences and labels\n",
        "        -add winow padding\n",
        "        -pad for lengths using pad_sequence\n",
        "        -convert our labels to one-hots\n",
        "        -return padded inputs, one-hot labels, and lengths\n",
        "    \"\"\"\n",
        "    \n",
        "    x_s, y_s = zip(*data)\n",
        "\n",
        "    # deal with input sentences as we've seen\n",
        "    window_padded = [convert_tokens_to_inds(pad_sentence_for_window(sentence, window_size), word_2_id)\n",
        "                                                                                  for sentence in x_s]\n",
        "    # append zeros to each list of token ids in batch so that they are all the same length\n",
        "    padded = nn.utils.rnn.pad_sequence([torch.LongTensor(t) for t in window_padded], batch_first=True)\n",
        "    \n",
        "    # convert labels to one-hots\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for y in y_s:\n",
        "        lengths.append(len(y))\n",
        "        one_hot = torch.zeros(len(y), len(train_id_2_label))\n",
        "        y = torch.tensor(y)\n",
        "        y = y.unsqueeze(1)\n",
        "        label = one_hot.scatter_(1, y, 1)\n",
        "        labels.append(label)\n",
        "    padded_labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "    \n",
        "    return padded.long(), padded_labels, torch.LongTensor(lengths)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sAza2qj-WKF"
      },
      "source": [
        "class SoftmaxWordWindowClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A one-layer, binary word-window classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, vocab_size, pad_idx=0):\n",
        "        super(SoftmaxWordWindowClassifier, self).__init__()\n",
        "        \"\"\"\n",
        "        Instance variables.\n",
        "        \"\"\"\n",
        "        self.window_size = 2*config[\"half_window\"]+1\n",
        "        self.embed_dim = config[\"embed_dim\"]\n",
        "        self.hidden_dim = config[\"hidden_dim\"]\n",
        "        self.num_classes = config[\"num_classes\"]\n",
        "        self.freeze_embeddings = config[\"freeze_embeddings\"]\n",
        "        \n",
        "        \"\"\"\n",
        "        Embedding layer\n",
        "        -model holds an embedding for each layer in our vocab\n",
        "        -sets aside a special index in the embedding matrix for padding vector (of zeros)\n",
        "        -by default, embeddings are parameters (so gradients pass through them)\n",
        "        \"\"\"\n",
        "        self.embed_layer = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_idx)\n",
        "        if self.freeze_embeddings:\n",
        "            self.embed_layer.weight.requires_grad = False\n",
        "        \n",
        "        \"\"\"\n",
        "        Hidden layer\n",
        "        -we want to map embedded word windows of dim (window_size+1)*self.embed_dim to a hidden layer.\n",
        "        -nn.Sequential allows you to efficiently specify sequentially structured models\n",
        "            -first the linear transformation is evoked on the embedded word windows\n",
        "            -next the nonlinear transformation tanh is evoked.\n",
        "        \"\"\"\n",
        "        self.hidden_layer = nn.Sequential(nn.Linear(self.window_size*self.embed_dim, \n",
        "                                                    self.hidden_dim), \n",
        "                                          nn.Tanh())\n",
        "        \n",
        "        \"\"\"\n",
        "        Output layer\n",
        "        -we want to map elements of the output layer (of size self.hidden dim) to a number of classes.\n",
        "        \"\"\"\n",
        "        self.output_layer = nn.Linear(self.hidden_dim, self.num_classes)\n",
        "        \n",
        "        \"\"\"\n",
        "        Softmax\n",
        "        -The final step of the softmax classifier: mapping final hidden layer to class scores.\n",
        "        -pytorch has both logsoftmax and softmax functions (and many others)\n",
        "        -since our loss is the negative LOG likelihood, we use logsoftmax\n",
        "        -technically you can take the softmax, and take the log but PyTorch's implementation\n",
        "         is optimized to avoid numerical underflow issues.\n",
        "        \"\"\"\n",
        "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Let B:= batch_size\n",
        "            L:= window-padded sentence length\n",
        "            D:= self.embed_dim\n",
        "            S:= self.window_size\n",
        "            H:= self.hidden_dim\n",
        "            \n",
        "        inputs: a (B, L) tensor of token indices\n",
        "        \"\"\"\n",
        "        B, L = inputs.size()\n",
        "        \n",
        "        \"\"\"\n",
        "        Reshaping.\n",
        "        Takes in a (B, L) LongTensor\n",
        "        Outputs a (B, L~, S) LongTensor\n",
        "        \"\"\"\n",
        "        # Fist, get our word windows for each word in our input.\n",
        "        token_windows = inputs.unfold(1, self.window_size, 1)\n",
        "        _, adjusted_length, _ = token_windows.size()\n",
        "        \n",
        "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
        "        assert token_windows.size() == (B, adjusted_length, self.window_size)\n",
        "        \n",
        "        \"\"\"\n",
        "        Embedding.\n",
        "        Takes in a torch.LongTensor of size (B, L~, S) \n",
        "        Outputs a (B, L~, S, D) FloatTensor.\n",
        "        \"\"\"\n",
        "        embedded_windows = self.embed_layer(token_windows)\n",
        "        \n",
        "        \"\"\"\n",
        "        Reshaping.\n",
        "        Takes in a (B, L~, S, D) FloatTensor.\n",
        "        Resizes it into a (B, L~, S*D) FloatTensor.\n",
        "        -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
        "        \"\"\"\n",
        "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
        "        \n",
        "        \"\"\"\n",
        "        Layer 1.\n",
        "        Takes in a (B, L~, S*D) FloatTensor.\n",
        "        Resizes it into a (B, L~, H) FloatTensor\n",
        "        \"\"\"\n",
        "        layer_1 = self.hidden_layer(embedded_windows)\n",
        "        \n",
        "        \"\"\"\n",
        "        Layer 2\n",
        "        Takes in a (B, L~, H) FloatTensor.\n",
        "        Resizes it into a (B, L~, 2) FloatTensor.\n",
        "        \"\"\"\n",
        "        output = self.output_layer(layer_1)\n",
        "        \n",
        "        \"\"\"\n",
        "        Softmax.\n",
        "        Takes in a (B, L~, 2) FloatTensor of unnormalized class scores.\n",
        "        Outputs a (B, L~, 2) FloatTensor of (log-)normalized class scores.\n",
        "        \"\"\"\n",
        "        output = self.log_softmax(output)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyATHYB5-5nz"
      },
      "source": [
        "def loss_function(outputs, labels, lengths):\n",
        "    \"\"\"Computes negative LL loss on a batch of model predictions.\"\"\"\n",
        "    B, L, num_classes = outputs.size()\n",
        "    num_elems = lengths.sum().float()\n",
        "        \n",
        "    # get only the values with non-zero labels\n",
        "    loss = outputs*labels\n",
        "    \n",
        "    # rescale average\n",
        "    return -loss.sum() / num_elems\n",
        "\n",
        "def train_epoch(loss_function, optimizer, model, train_data):\n",
        "    \n",
        "    ## For each batch, we must reset the gradients\n",
        "    ## stored by the model.    \n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch, labels, lengths in train_data:\n",
        "        batch, labels, lengths = batch.to(device), labels.to(device), lengths.to(device)\n",
        "        # clear gradients\n",
        "        optimizer.zero_grad()\n",
        "        # evoke model in training mode on batch\n",
        "        outputs = model.forward(batch)\n",
        "        # compute loss w.r.t batch\n",
        "        loss = loss_function(outputs, labels, lengths)\n",
        "        # pass gradients back, startiing on loss value\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # return the total to keep track of how you did this time around\n",
        "    return total_loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pHbaCp51Az_"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdCbC3er-9GM"
      },
      "source": [
        "config = {\"batch_size\": 4,\n",
        "          \"half_window\": 2,\n",
        "          \"embed_dim\": 25,\n",
        "          \"hidden_dim\": 25,\n",
        "          \"num_classes\": 13, \n",
        "          \"freeze_embeddings\": False,\n",
        "          'lr': 0.02,\n",
        "          'epochs': 100,\n",
        "         }\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def train(model, config):\n",
        "    learning_rate = config['lr'] # org: 0.0002\n",
        "    num_epochs = config['epochs']\n",
        "\n",
        "    # Shuffle True is good practice for train loaders.\n",
        "    # Use functools.partial to construct a partially populated collate function\n",
        "    train_loader = DataLoader(list(zip(train_sent_tokens, train_label_inds)), \n",
        "                                batch_size=config['batch_size'], shuffle=True, \n",
        "                                collate_fn=partial(my_collate, window_size=config['half_window'], word_2_id=word_2_id))    \n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5) # step=30, gamma=0.1 for epochs=100\n",
        "\n",
        "    # TRAIN\n",
        "    losses = []\n",
        "    lr = []\n",
        "    for epoch in range(num_epochs):    \n",
        "        epoch_loss = train_epoch(loss_function, optimizer, model, train_loader)\n",
        "        lr.append(scheduler.get_last_lr()[0])\n",
        "        losses.append([epoch_loss])\n",
        "        if epoch%20 == 0:\n",
        "            print(epoch, epoch_loss)\n",
        "        scheduler.step()\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
        "    axes[0].plot(range(num_epochs), lr)\n",
        "    axes[0].set_title('learning rate')\n",
        "    axes[1].plot(range(num_epochs), losses)\n",
        "    axes[1].set_title('training loss')\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1rTPZHVRrpz"
      },
      "source": [
        "### Evaluation\n",
        "There is much more O entities in data set, but we’re more interested in other entities. To account for this we’ll use averaged F1 score computed for all labels except for O. sklearn-crfsuite.metrics package provides some useful metrics for sequence classification task, including this one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei4N1XO9czHO"
      },
      "source": [
        "def eval(model, test_sent_tokens, test_label_inds, test_labels, config):    \n",
        "    test_loader = DataLoader(list(zip(test_sent_tokens, test_label_inds)), \n",
        "                                batch_size=config['batch_size'], shuffle=False, \n",
        "                                collate_fn=partial(my_collate, window_size=config['half_window'], word_2_id=word_2_id))\n",
        "\n",
        "    test_outputs = []\n",
        "    for test_instance, labs, _ in test_loader:\n",
        "        test_instance, labs = test_instance.to(device), labs.to(device)\n",
        "        outputs_full = model.forward(test_instance)\n",
        "        outputs = torch.argmax(outputs_full, dim=2)\n",
        "        for i in range(outputs.size(0)):\n",
        "            test_outputs.append(outputs[i].tolist())\n",
        "\n",
        "    y_test = test_labels\n",
        "    y_pred = []\n",
        "    for test, pred in zip(test_labels, test_outputs):\n",
        "        y_pred.append([train_id_2_label[id] for id in pred[:len(test)]])\n",
        "\n",
        "    assert len(y_pred) == len(y_test), '{} vs. {}'.format(len(y_pred), len(y_test))\n",
        "    for i, pred, test in zip(list(range(len(y_pred))), y_pred, y_test):\n",
        "        assert len(pred) == len(test), '{}: {} vs. {}'.format(i, len(pred), len(test))\n",
        "\n",
        "    f1 = metrics.flat_f1_score(y_test, y_pred, average='weighted', labels=train_id_2_label)\n",
        "    print('check f1 score',f1)\n",
        "    idx = randint(0, len(test_labels)-1)\n",
        "    print('check random predictions')\n",
        "    print('true labels: ', test_labels[idx])\n",
        "    print('predicted labels: ',y_pred[idx])\n",
        "    return f1, y_test, y_pred"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thb1I8SRRrp9"
      },
      "source": [
        "### Inspect per-class results in more detail:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLsATtVc18MN"
      },
      "source": [
        "### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC9niXA72o2i"
      },
      "source": [
        "# restart kernel so cleared outputs\n",
        "params = {\n",
        "'batch_size': 4,\n",
        " 'embed_dim': [100, 200, 300],\n",
        " 'freeze_embeddings': False,\n",
        " 'half_window': [3, 4, 5],\n",
        " 'hidden_dim': [50, 100],\n",
        " 'num_classes': 13,\n",
        " 'lr': [0.1],\n",
        "}\n",
        "\n",
        "params2tune = ['embed_dim', 'hidden_dim', 'lr', 'half_window']\n",
        "for param in params2tune:\n",
        "    temp = copy.copy(config)\n",
        "    for val in params[param]:\n",
        "        temp[param] = val\n",
        "        print(param, val)\n",
        "        model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
        "        model = train(model, temp)\n",
        "        f1, _, _ = eval(model, dev_sent_tokens, dev_label_inds, dev_labels, temp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-Q6KDmRalUx"
      },
      "source": [
        "### Train with optimal params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bD87ovm3QPfO",
        "outputId": "a08ec39e-edb3-45cd-fe9c-33677058e2f4"
      },
      "source": [
        "sorted_labels = sorted(\n",
        "    train_id_2_label,\n",
        "    key=lambda name: (name[1:], name[0])\n",
        ")\n",
        "\n",
        "temp = copy.copy(config)\n",
        "temp['embed_dim'] = 100\n",
        "temp['epochs'] = 1000\n",
        "temp['lr'] = 0.02\n",
        "print(temp)\n",
        "model = SoftmaxWordWindowClassifier(config, len(word_2_id))\n",
        "model = train(model, temp)\n",
        "print('dev evaluation')\n",
        "f1, _, _ = eval(model, dev_sent_tokens, dev_label_inds, dev_labels, temp)\n",
        "print('test evaluation')\n",
        "f1, y_test, y_pred = eval(model, test_sent_tokens, test_label_inds, test_labels, temp)\n",
        "\n",
        "print(metrics.flat_classification_report(\n",
        "    y_test, y_pred, labels=sorted_labels, digits=3\n",
        "))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'batch_size': 4, 'half_window': 2, 'embed_dim': 100, 'hidden_dim': 25, 'num_classes': 13, 'freeze_embeddings': False, 'lr': 0.02, 'epochs': 1000}\n",
            "0 341.0711578093469\n",
            "20 158.5276240594685\n",
            "40 147.8348139990121\n",
            "60 136.3057412393391\n",
            "80 127.36744286306202\n",
            "100 118.86299353837967\n",
            "120 115.63777966424823\n",
            "140 111.3352690115571\n",
            "160 109.49670790042728\n",
            "180 106.3065825952217\n",
            "200 102.31963225826621\n",
            "220 100.98802460543811\n",
            "240 99.95833320869133\n",
            "260 98.64964246377349\n",
            "280 96.69916796218604\n",
            "300 96.33948220126331\n",
            "320 94.44644452538341\n",
            "340 93.56727128662169\n",
            "360 94.3340802770108\n",
            "380 94.31676530558616\n",
            "400 92.28549716062844\n",
            "420 92.23177853925154\n",
            "440 92.08689666353166\n",
            "460 90.1944078207016\n",
            "480 90.98448238102719\n",
            "500 90.71044348413125\n",
            "520 91.22237672284245\n",
            "540 88.9292311258614\n",
            "560 89.40400730445981\n",
            "580 89.85433993628249\n",
            "600 89.53317624609917\n",
            "620 89.51861200574785\n",
            "640 89.9917972208932\n",
            "660 89.15616011852399\n",
            "680 89.84686810523272\n",
            "700 89.54060405213386\n",
            "720 88.89983741892502\n",
            "740 89.58677281020209\n",
            "760 88.93170772492886\n",
            "780 89.90778724849224\n",
            "800 88.75482221366838\n",
            "820 89.96458820672706\n",
            "840 89.63242335198447\n",
            "860 89.17737675225362\n",
            "880 88.54574660677463\n",
            "900 89.71192811243236\n",
            "920 90.92745565064251\n",
            "940 88.99332577595487\n",
            "960 90.39386763004586\n",
            "980 88.23434047400951\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "dev evaluation\n",
            "check f1 score 0.8797339184825274\n",
            "check random predictions\n",
            "true labels:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "predicted labels:  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "test evaluation\n",
            "check f1 score 0.9510608524694358\n",
            "check random predictions\n",
            "true labels:  ['O', 'O', 'O', 'B-location', 'I-location', 'O', 'B-location']\n",
            "predicted labels:  ['O', 'O', 'O', 'B-location', 'I-location', 'O', 'B-location']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "                 precision    recall  f1-score   support\n",
            "\n",
            "              O      0.968     0.999     0.983     44296\n",
            "  B-corporation      0.744     0.362     0.487       177\n",
            "  I-corporation      0.000     0.000     0.000        36\n",
            "B-creative-work      0.458     0.103     0.168       107\n",
            "I-creative-work      0.529     0.214     0.305       168\n",
            "        B-group      0.692     0.061     0.112       148\n",
            "        I-group      0.538     0.065     0.116       108\n",
            "     B-location      0.731     0.437     0.547       391\n",
            "     I-location      0.635     0.201     0.306       164\n",
            "       B-person      0.512     0.184     0.270       468\n",
            "       I-person      0.532     0.147     0.230       225\n",
            "      B-product      0.600     0.030     0.057       100\n",
            "      I-product      0.455     0.062     0.109        81\n",
            "\n",
            "       accuracy                          0.962     46469\n",
            "      macro avg      0.569     0.220     0.284     46469\n",
            "   weighted avg      0.950     0.962     0.951     46469\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94RfbQpPflp-"
      },
      "source": [
        "torch.save(model.state_dict(), 'model_emb100_lr0.02_ep1000.pth')"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZke02eYfmiM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}